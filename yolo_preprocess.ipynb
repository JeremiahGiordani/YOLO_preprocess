{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Purpose\n",
        "\n",
        "This notebook demonstrates the differences between the source implementation and a branch implementation of a preprocessing function, explains why the branch implementation throws an error, and provides a recommended fix. We will walk through how each implementation handles object detection targets, highlighting key differences in how data is structured and processed.\n"
      ],
      "metadata": {
        "id": "KYnNAv0ZlDRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input data mimicking the format used by the source implementation\n",
        "\n",
        "# 'input_targets_source_implementation' is a tensor where each row corresponds to\n",
        "# a detected object in an image. The first column represents the image index,\n",
        "# the second column represents the object label, and the remaining four columns\n",
        "# represent the bounding box coordinates in [x_center, y_center, width, height] format.\n",
        "\n",
        "# Here, we are creating data for 4 unique images. Three of the images have one object each.\n",
        "# One image (Image 2) contains two objects, but there are only three unique labels across\n",
        "# all the images. The batch size will be greater than the number of unique labels.\n",
        "\n",
        "import torch\n",
        "\n",
        "# Creating the input data for the source implementation\n",
        "# Format: [image_index, label, x_center, y_center, width, height]\n",
        "targets_source_implementation = torch.tensor([\n",
        "    [0, 1, 0.1, 0.2, 0.3, 0.4],  # Image 0, Object 1 (Label 1)\n",
        "    [1, 2, 0.5, 0.6, 0.7, 0.8],  # Image 1, Object 1 (Label 2)\n",
        "    [2, 1, 0.3, 0.3, 0.4, 0.4],  # Image 2, Object 1 (Label 1)\n",
        "    [2, 3, 0.7, 0.7, 0.2, 0.2],  # Image 2, Object 2 (Label 3)\n",
        "    [3, 2, 0.4, 0.5, 0.6, 0.7],  # Image 3, Object 1 (Label 2)\n",
        "])\n",
        "\n",
        "# Explanation:\n",
        "# - Image 0 contains 1 object (Label 1) with bounding box [0.1, 0.2, 0.3, 0.4].\n",
        "# - Image 1 contains 1 object (Label 2) with bounding box [0.5, 0.6, 0.7, 0.8].\n",
        "# - Image 2 contains 2 objects:\n",
        "#     - Object 1: Label 1 with bounding box [0.3, 0.3, 0.4, 0.4].\n",
        "#     - Object 2: Label 3 with bounding box [0.7, 0.7, 0.2, 0.2].\n",
        "# - Image 3 contains 1 object (Label 2) with bounding box [0.4, 0.5, 0.6, 0.7].\n",
        "# - There are 4 images but only 3 unique labels: {1, 2, 3}.\n"
      ],
      "metadata": {
        "id": "mCPWXtBlc3wG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of the source code preprocess function\n",
        "def source_preprocess(targets, batch_size, scale_tensor):\n",
        "    if targets.shape[0] == 0:\n",
        "        out = torch.zeros(batch_size, 0, 5, device='cpu')\n",
        "    else:\n",
        "        i = targets[:, 0]  # image index\n",
        "        _, counts = i.unique(return_counts=True)\n",
        "        out = torch.zeros(batch_size, counts.max(), 5, device='cpu')\n",
        "        for j in range(batch_size):\n",
        "            matches = i == j\n",
        "            n = matches.sum()\n",
        "            if n:\n",
        "                out[j, :n] = targets[matches, 1:]\n",
        "        out[..., 1:5] = xywh2xyxy(out[..., 1:5])\n",
        "    return out\n",
        "\n",
        "# Dummy bounding box conversion function\n",
        "def xywh2xyxy(boxes):\n",
        "    # This is a placeholder function to simulate the bounding box conversion.\n",
        "    return boxes  # In reality, it does more."
      ],
      "metadata": {
        "id": "QqroO532c-78"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GApL7qiX_aDg",
        "outputId": "e5bf74ab-8c24-4391-9205-c24f6a570844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of source_preprocess function:\n",
            " tensor([[[1.0000, 0.1000, 0.2000, 0.3000, 0.4000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "        [[2.0000, 0.5000, 0.6000, 0.7000, 0.8000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "        [[1.0000, 0.3000, 0.3000, 0.4000, 0.4000],\n",
            "         [3.0000, 0.7000, 0.7000, 0.2000, 0.2000]],\n",
            "\n",
            "        [[2.0000, 0.4000, 0.5000, 0.6000, 0.7000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
            "\n",
            "Image 0 contains the following objects:\n",
            "  Object 1: Label = 1.0, Bounding Box = [0.10000000149011612, 0.20000000298023224, 0.30000001192092896, 0.4000000059604645]\n",
            "  Object 2: No object (empty row)\n",
            "\n",
            "Image 1 contains the following objects:\n",
            "  Object 1: Label = 2.0, Bounding Box = [0.5, 0.6000000238418579, 0.699999988079071, 0.800000011920929]\n",
            "  Object 2: No object (empty row)\n",
            "\n",
            "Image 2 contains the following objects:\n",
            "  Object 1: Label = 1.0, Bounding Box = [0.30000001192092896, 0.30000001192092896, 0.4000000059604645, 0.4000000059604645]\n",
            "  Object 2: Label = 3.0, Bounding Box = [0.699999988079071, 0.699999988079071, 0.20000000298023224, 0.20000000298023224]\n",
            "\n",
            "Image 3 contains the following objects:\n",
            "  Object 1: Label = 2.0, Bounding Box = [0.4000000059604645, 0.5, 0.6000000238418579, 0.699999988079071]\n",
            "  Object 2: No object (empty row)\n"
          ]
        }
      ],
      "source": [
        "# Testing the source_preprocess function using the input_targets_source_implementation\n",
        "\n",
        "# Set up parameters for the test\n",
        "batch_size = 4  # We have 4 images, so the batch size is set to 4 (greater than the number of unique labels)\n",
        "scale_tensor = torch.tensor([640, 480, 640, 480])\n",
        "\n",
        "# Run the preprocess function\n",
        "output = source_preprocess(targets_source_implementation, batch_size, scale_tensor)\n",
        "\n",
        "# Print the output tensor\n",
        "print(\"Output of source_preprocess function:\\n\", output)\n",
        "\n",
        "# Explanation:\n",
        "# The output tensor has the shape [batch_size, max_objects_per_image, 5].\n",
        "# In this case:\n",
        "# - The first dimension corresponds to the 4 unique images in the batch.\n",
        "# - The second dimension represents the maximum number of objects in any image (in this case, 2 objects for Image 2).\n",
        "# - The third dimension holds 5 values for each object:\n",
        "#     - The first value is the object label.\n",
        "#     - The next four values are the bounding box coordinates in [x_min, y_min, x_max, y_max] format (after conversion).\n",
        "\n",
        "# Detailed breakdown of the output:\n",
        "for img_idx in range(batch_size):\n",
        "    print(f\"\\nImage {img_idx} contains the following objects:\")\n",
        "    for obj_idx in range(output.shape[1]):\n",
        "        obj_data = output[img_idx, obj_idx]\n",
        "        if obj_data[0] != 0:  # Skip empty entries (padded with zeros)\n",
        "            print(f\"  Object {obj_idx + 1}: Label = {obj_data[0].item()}, \"\n",
        "                  f\"Bounding Box = {obj_data[1:].tolist()}\")\n",
        "        else:\n",
        "            print(f\"  Object {obj_idx + 1}: No object (empty row)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Branch Implementation Targets\n",
        "\n",
        "In this section, we create the `targets_branch_implementation`, which represents the target data as a list of dictionaries. Each dictionary corresponds to an object in an image, containing the image index, object label, and bounding box. This structure is different from the source implementation, but it stores the same information in a more flexible format.\n"
      ],
      "metadata": {
        "id": "i4xiXYJ9lpTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating 'targets_branch_implementation' as a list of dictionaries\n",
        "\n",
        "# This format is different from the source implementation because it stores the data as a list of dictionaries.\n",
        "# Each dictionary corresponds to a detected object in an image and contains fields for:\n",
        "# - \"image\": Identifies the image index (similar to the first column in the source format).\n",
        "# - \"labels\": The label of the object detected in the image (scalar).\n",
        "# - \"boxes\": The bounding box coordinates for the object in the image.\n",
        "\n",
        "# I made the assumption that this structure contains an \"image\" field because the model needs to associate\n",
        "# each label and bounding box with a specific image, especially when multiple images and multiple objects\n",
        "# per image are involved. Without this, it would be difficult to match labels and boxes to their corresponding images.\n",
        "\n",
        "targets_branch_implementation = [\n",
        "    {\"image\": torch.tensor([0]), \"labels\": torch.tensor([1]), \"boxes\": torch.tensor([[0.1, 0.2, 0.3, 0.4]])},  # Image 0, 1 object\n",
        "    {\"image\": torch.tensor([1]), \"labels\": torch.tensor([2]), \"boxes\": torch.tensor([[0.5, 0.6, 0.7, 0.8]])},  # Image 1, 1 object\n",
        "    {\"image\": torch.tensor([2]), \"labels\": torch.tensor([1]), \"boxes\": torch.tensor([[0.3, 0.3, 0.4, 0.4]])},  # Image 2, Object 1\n",
        "    {\"image\": torch.tensor([2]), \"labels\": torch.tensor([3]), \"boxes\": torch.tensor([[0.7, 0.7, 0.2, 0.2]])},  # Image 2, Object 2\n",
        "    {\"image\": torch.tensor([3]), \"labels\": torch.tensor([2]), \"boxes\": torch.tensor([[0.4, 0.5, 0.6, 0.7]])},  # Image 3, 1 object\n",
        "]\n",
        "\n",
        "# Explanation:\n",
        "# - Each entry now corresponds to a single object, even if multiple objects are in the same image.\n",
        "# - The structure is a list of dictionaries where:\n",
        "#     - \"image\": A scalar tensor identifying the image index.\n",
        "#     - \"labels\": A scalar tensor representing the label for each object.\n",
        "#     - \"boxes\": A tensor of bounding box coordinates for the object.\n",
        "# - For Image 2, we have split the objects into two separate rows: one for Label 1 and one for Label 3.\n",
        "# - This adjustment ensures that 'labels' is always a scalar, aligning with the expected data structure.\n"
      ],
      "metadata": {
        "id": "UC_tzROmADME"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Original branch preprocess implementation (this will throw an error)\n",
        "def branch_preprocess(targets, batch_size, scale_tensor):\n",
        "    \"\"\"Preprocesses targets before computing loss.\"\"\"\n",
        "    if len(targets) == 0:\n",
        "        out = torch.zeros(batch_size, 0, 5, device='cpu')\n",
        "    else:\n",
        "        labels = []\n",
        "        for label in targets:\n",
        "            labels.append(label[\"labels\"])\n",
        "\n",
        "        _, counts = torch.tensor(labels).unique(return_counts=True)\n",
        "\n",
        "        # The tensor is size 5 because of (class, x, y, x, y)\n",
        "        out = torch.zeros(batch_size, counts.max(), 5, device='cpu')\n",
        "\n",
        "        for batch in range(batch_size):\n",
        "                out[batch, : counts[batch], 0] = targets[batch][\"labels\"]\n",
        "                out[batch, : counts[batch], 1:5] = targets[batch][\"boxes\"]\n",
        "        out[..., 1:5] = xywh2xyxy(out[..., 1:5])\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# Test with branch implementation (this will throw an error)\n",
        "try:\n",
        "    batch_size = 4  # Batch size is set to 4 for this test\n",
        "    scale_tensor = torch.tensor([640, 480, 640, 480])  # Dummy scaling factors\n",
        "    output_branch = branch_preprocess(targets_branch_implementation, batch_size, scale_tensor)\n",
        "\n",
        "    # Print the output if no error occurs (though an error is expected)\n",
        "    print(\"Output of branch_preprocess function:\\n\", output_branch)\n",
        "except Exception as e:\n",
        "    # Print the error message\n",
        "    print(f\"Error occurred: {e}\")\n",
        "\n",
        "# Explanation:\n",
        "# This implementation throws an error because of the line `out[batch, :counts[batch]]`.\n",
        "# The issue arises when the batch size is greater than the number of unique labels in the data.\n",
        "# Specifically, the variable `counts` contains the count of unique labels, and its length\n",
        "# (i.e., the number of unique labels, `len(counts)`) is less than the batch size.\n",
        "# When the loop tries to access `counts[batch]` for batches that don't have a corresponding\n",
        "# unique label, it causes an out-of-bounds error.\n",
        "# This happens because there aren't enough unique labels to match the batch size, leading\n",
        "# to an attempt to index into `counts` for a batch index that doesn't exist.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0XNRdO8f7ZR",
        "outputId": "2fb8b201-950c-49f4-91e7-6cf1e2114002"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: index 3 is out of bounds for dimension 0 with size 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fixing the Branch Implementation: Converting Targets to Match Source Format\n",
        "\n",
        "In this section, we implement the `preprocess_fixed_branch` function to fix the issues in the branch implementation. The logic behind the fix is to convert the target data from a list of dictionaries into a single tensor, matching the format of the source implementation. By doing so, we ensure that the data can be processed using the same logic as the source code. Each object in an image is represented by a row in the tensor, containing the image index, object label, and bounding box, which allows the function to handle multiple objects per image and avoid errors related to mismatched batch size and label counts.\n"
      ],
      "metadata": {
        "id": "vi1i8bFamBIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified branch preprocess function that processes 'targets' into the tensor shape of the original implementation\n",
        "def preprocess_fixed_branch(targets, batch_size, scale_tensor):\n",
        "    \"\"\"Preprocesses targets and converts them into the tensor format expected by the original implementation.\"\"\"\n",
        "\n",
        "    # Step 1: Convert the list of dictionaries (targets) into a single tensor, similar to the source implementation\n",
        "    all_targets = []\n",
        "\n",
        "    for target in targets:\n",
        "        image_idx = target[\"image\"]  # Extract the image index\n",
        "        labels = target[\"labels\"]  # Scalar label\n",
        "        boxes = target[\"boxes\"]  # Bounding box\n",
        "\n",
        "        # Stack the image index, label, and bounding box into a tensor for each object\n",
        "        img_target = torch.cat([\n",
        "            image_idx.unsqueeze(0).float(),  # Image index\n",
        "            labels.unsqueeze(0).float(),                         # Labels (convert to 2D for concatenation)\n",
        "            boxes.float()                                        # Bounding box\n",
        "        ], dim=1)\n",
        "\n",
        "        all_targets.append(img_target)\n",
        "\n",
        "    # Step 2: Concatenate all individual object tensors into one large tensor, like the source implementation\n",
        "    all_targets = torch.cat(all_targets, dim=0)  # Shape: (total_objects, 6)\n",
        "    print(\"You can see now that the shape of targets after modification matches the shape of targest in the source implementation\")\n",
        "    print(all_targets)\n",
        "\n",
        "    # Step 3: Now we can process 'all_targets' just like in the original source implementation\n",
        "    if all_targets.shape[0] == 0:\n",
        "        out = torch.zeros(batch_size, 0, 5, device='cpu')\n",
        "    else:\n",
        "        i = all_targets[:, 0]  # Image index\n",
        "        _, counts = i.unique(return_counts=True)\n",
        "        out = torch.zeros(batch_size, counts.max(), 5, device='cpu')\n",
        "\n",
        "        for j in range(batch_size):\n",
        "            matches = i == j  # Find the targets for the current image (j)\n",
        "            n = matches.sum()  # Number of objects in this image\n",
        "            if n:\n",
        "                out[j, :n] = all_targets[matches, 1:]  # Set labels and bounding boxes for this image\n",
        "\n",
        "        out[..., 1:5] = xywh2xyxy(out[..., 1:5])\n",
        "\n",
        "    return out\n",
        "\n",
        "# Dummy bounding box conversion function (unchanged)\n",
        "def xywh2xyxy(boxes):\n",
        "    # This is a placeholder function to simulate the bounding box conversion.\n",
        "    return boxes  # In reality, you'd convert xywh format to xyxy format here.\n",
        "\n",
        "# Running the test with the fixed branch implementation\n",
        "try:\n",
        "    batch_size = 4  # Batch size of 4 images\n",
        "    scale_tensor = torch.tensor([640, 480, 640, 480])  # Dummy scaling factors\n",
        "    output_fixed_branch = preprocess_fixed_branch(targets_branch_implementation, batch_size, scale_tensor)\n",
        "\n",
        "    # Print the output\n",
        "    print(\"Output of preprocess_fixed_branch function:\\n\", output_fixed_branch)\n",
        "    print(\"You can now see that the output matches the output of the source implementation on comparable inputs\")\n",
        "except Exception as e:\n",
        "    # Print the error if something goes wrong\n",
        "    print(f\"Error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUdAweNrhw0Q",
        "outputId": "5bce9ffd-7008-44b2-888e-0faf9a55f59c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You can see now that the shape of targets after modification matches the shape of targest in the source implementation\n",
            "tensor([[0.0000, 1.0000, 0.1000, 0.2000, 0.3000, 0.4000],\n",
            "        [1.0000, 2.0000, 0.5000, 0.6000, 0.7000, 0.8000],\n",
            "        [2.0000, 1.0000, 0.3000, 0.3000, 0.4000, 0.4000],\n",
            "        [2.0000, 3.0000, 0.7000, 0.7000, 0.2000, 0.2000],\n",
            "        [3.0000, 2.0000, 0.4000, 0.5000, 0.6000, 0.7000]])\n",
            "Output of preprocess_fixed_branch function:\n",
            " tensor([[[1.0000, 0.1000, 0.2000, 0.3000, 0.4000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "        [[2.0000, 0.5000, 0.6000, 0.7000, 0.8000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "        [[1.0000, 0.3000, 0.3000, 0.4000, 0.4000],\n",
            "         [3.0000, 0.7000, 0.7000, 0.2000, 0.2000]],\n",
            "\n",
            "        [[2.0000, 0.4000, 0.5000, 0.6000, 0.7000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
            "You can now see that the output matches the output of the source implementation on comparable inputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "with open(os.devnull, 'w') as devnull:\n",
        "    with redirect_stdout(devnull):\n",
        "        output_fixed_branch = preprocess_fixed_branch(targets_branch_implementation, batch_size, scale_tensor)\n",
        "\n",
        "print(output_fixed_branch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Dxax9UnkFwc",
        "outputId": "0e1a67df-3aef-4210-a503-ce8774046d69"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1.0000, 0.1000, 0.2000, 0.3000, 0.4000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "        [[2.0000, 0.5000, 0.6000, 0.7000, 0.8000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "        [[1.0000, 0.3000, 0.3000, 0.4000, 0.4000],\n",
            "         [3.0000, 0.7000, 0.7000, 0.2000, 0.2000]],\n",
            "\n",
            "        [[2.0000, 0.4000, 0.5000, 0.6000, 0.7000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_source_implementation = source_preprocess(targets_source_implementation, batch_size, scale_tensor)\n",
        "print(output_source_implementation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHmAmvO1kGOo",
        "outputId": "ee028fdb-7b82-4ca3-eb0a-d74462984953"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1.0000, 0.1000, 0.2000, 0.3000, 0.4000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "        [[2.0000, 0.5000, 0.6000, 0.7000, 0.8000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "        [[1.0000, 0.3000, 0.3000, 0.4000, 0.4000],\n",
            "         [3.0000, 0.7000, 0.7000, 0.2000, 0.2000]],\n",
            "\n",
            "        [[2.0000, 0.4000, 0.5000, 0.6000, 0.7000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n"
          ]
        }
      ]
    }
  ]
}